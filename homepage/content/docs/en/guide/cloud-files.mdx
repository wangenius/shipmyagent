---
title: "Cloud Files (OSS / public)"
description: "Two ways to expose files via URL: upload to object storage or serve from /public/*"
---

# Cloud Files (OSS / public)

In some workflows you need a URL that external tools/services can fetch, for example:

- Let an external service pull a PDF/image/zip via URL
- Share run artifacts with teammates
- Publish an Agent output as a downloadable link

ShipMyAgent supports two approaches:

1. **Configure OSS (S3-compatible)**: upload project-local files to object storage (Cloudflare R2 is S3-compatible).
2. **Deploy on a server and use `/public/*`**: put files under `.ship/public/` and let the server expose them via the `/public/*` route.

ShipMyAgent also provides an Agent-facing “cloud files” toolset with a default strategy:

- Prefer **OSS** when configured
- Otherwise fall back to **public** when `cloudFiles.publicBaseUrl` is configured
- If neither is configured, upload fails with “no upload path configured”

---

## Option 1: Configure OSS (enable `s3_upload`)

### 1) Configure `oss` in `ship.json`

Use env placeholders so you don’t commit secrets. Also set a default bucket so higher-level tools can simply call `upload(filePath)`:

```json
{
  "oss": {
    "enabled": true,
    "provider": "s3",
    "endpoint": "${R2_S3_ENDPOINT}",
    "accessKeyId": "${R2_ID}",
    "secretAccessKey": "${R2_SECRET_KEY}",
    "region": "auto",
    "bucket": "my-bucket"
  }
}
```

Notes:

- `endpoint`: S3 endpoint (R2 example: `https://<account-id>.r2.cloudflarestorage.com`)
- `region`: SigV4 region (R2 typically uses `auto`)
- If `oss` is not fully configured, the runtime **will not register** the `s3_upload` tool

### 2) Provide secrets via `.env` (local/server environment)

```bash
R2_ID="..."
R2_SECRET_KEY="..."
R2_S3_ENDPOINT="https://<account-id>.r2.cloudflarestorage.com"
```

### 3) Ask the Agent to upload

For example:

- “Upload `dist/report.pdf` to bucket `my-bucket` with key `reports/report.pdf`”

Equivalent tool input:

```json
{
  "bucket": "my-bucket",
  "file": "dist/report.pdf",
  "key": "reports/report.pdf",
  "contentType": "application/pdf"
}
```

> `file` must point to a file inside the project root (relative path from the project root).

---

## Option 2: Deploy on a server and serve `.ship/public/` via `/public/*`

When ShipMyAgent runs on a server, the main API server exposes:

- URL: `/public/<path>`
- Disk path: `.ship/public/<path>`

### 1) Place the file

```bash
mkdir -p .ship/public
cp /path/to/report.pdf .ship/public/report.pdf
```

### 2) Access the URL

```text
https://<host>:<port>/public/report.pdf
```

If you enable the interactive web server (interactive port), it also proxies `/public/*` to the main server, so the same path works there too.

---

## Cloud file tools (for the Agent)

### Upload: `cloud_file_upload`

Minimal usage:

```json
{ "filePath": "dist/report.pdf" }
```

The result includes:

- `method`: `oss` or `public`
- `url`: an accessible link

If neither `oss` (with a default bucket) nor `cloudFiles.publicBaseUrl` is configured, it returns a failure (“no upload path configured”).

### Build URL: `cloud_file_url`

Converts a known `bucket/key` or `publicPath` into a URL:

```json
{ "method": "public", "publicPath": "uploads/20260101-report.pdf" }
```

### Delete: `cloud_file_delete`

```json
{ "method": "public", "publicPath": "uploads/20260101-report.pdf" }
```

## Security notes

- `/public/*` is a public file-serving surface: do not put secrets (keys/tokens/internal data) there.
- Prefer adding auth / IP allowlist at your reverse proxy (Nginx/Caddy) or only exposing it on a private network.
- The server only allows `GET`/`HEAD` and normalizes paths to prevent reading files outside the project.
