# 肯德基小红书用户画像调研项目计划

> **目标**: 通过小红书平台采集1000名肯德基相关用户的公开数据，构建结构化用户画像  
> **版本**: v2.0 (讨论稿)  
> **更新**: 2026-02-06

---

## 一、项目概述

### 1.1 用户定义 (User Definition)

**核心定义**: 在小红书平台上发布过与肯德基(KFC)相关内容笔记的真实用户

**判定标准**:
- 笔记内容包含 "肯德基"/"KFC"/"肯德基疯狂星期四" 等关键词
- 发布过探店、测评、打卡、薅羊毛攻略等内容
- 排除官方账号、营销号、僵尸号

### 1.2 项目产出物

1. `raw_users.csv` - 初筛用户池 (N > 2000)
2. `filtered_users.csv` - 有效用户数据集 (N = 1000)
3. `persona_report.md` - 用户画像分析报告
4. `persona_visualization.html` - 可视化仪表盘

---

## 二、算法策略

### 2.1 用户评分体系 (Scoring System)

采用多维度加权评分模型，总分100分：

| 维度 | 权重 | 评分细则 | 数据来源 |
|------|------|----------|----------|
| **内容质量** | 35% | 图文质量、信息量、原创度(0-35) | 笔记详情 |
| **活跃度** | 25% | 笔记频率、互动回复率(0-25) | 主页数据 |
| **影响力** | 20% | 粉丝数、获赞收藏总量(0-20) | 主页数据 |
| **垂直度** | 20% | 美食/探店内容占比(0-20) | 笔记主题分析 |

**过滤阈值**:
- 基础门槛: 总分 ≥ 40分
- 优先录取: 总分 ≥ 70分 (标记为"高质量用户")
- 黑名单: 总分 < 20分 (营销号/僵尸号特征)

### 2.2 用户分层 (Segmentation)

采集后按以下标签分类：

| 标签 | 定义 | 用途 |
|------|------|------|
| `hardcore_fan` | KFC内容占比>30%，发布>10篇 | 核心粉丝画像 |
| `occasional` | 偶尔发布KFC相关内容 | 普通消费者画像 |
| `bargain_hunter` | 专注薅羊毛、优惠信息 | 价格敏感型用户 |
| `content_creator` | 粉丝>1万，内容专业 | KOC/KOL画像 |
| `lifestyle` | 融入生活方式场景 | 场景化营销洞察 |

### 2.3 画像维度 (Persona Dimensions)

最终输出包含以下字段：

**基础信息**: 性别推断、年龄段推断、地域、职业标签
**行为特征**: 发布习惯(时段/频率)、互动风格、内容偏好
**消费洞察**: 价格敏感度、新品尝试意愿、场景偏好
**内容风格**: 视觉风格、文案风格、话题偏好

---

## 三、技术策略

### 3.1 数据流转架构

```
┌─────────────────────────────────────────────────────────────┐
│                    Phase 1: 用户发现 (Discovery)              │
├─────────────────────────────────────────────────────────────┤
│  搜索词: "肯德基", "KFC", "疯狂星期四", "肯德基新品"            │
│         ↓                                                   │
│  采集笔记列表 → 提取作者ID → 去重 → raw_candidates.json      │
│         ↓                                                   │
│  目标: 收集 2000+ 候选用户ID                                  │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                    Phase 2: 主页采集 (Crawling)               │
├─────────────────────────────────────────────────────────────┤
│  逐个访问用户主页 → 采集:                                     │
│    - 基础信息: 昵称、简介、头像、粉丝数、关注数、获赞数         │
│    - 笔记列表: 最近20篇笔记的标题、封面、互动数据              │
│    - 笔记详情: 正文内容、图片数量、发布时间、标签              │
│         ↓                                                   │
│  存储 → raw_users.csv (N=50 for test / N=2000+ for prod)    │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                    Phase 3: 智能评估 (Evaluation)             │
├─────────────────────────────────────────────────────────────┤
│  算法评分 → 人工抽检验证 → 分层标记                           │
│         ↓                                                   │
│  存储 → filtered_users.csv (N=1000)                         │
└─────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                    Phase 4: 画像生成 (Analysis)               │
├─────────────────────────────────────────────────────────────┤
│  数据清洗 → 聚类分析 → 标签提取 → 可视化报告                   │
│         ↓                                                   │
│  产出 → persona_report.md + persona_dashboard.html          │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 初筛逻辑 (Pre-filtering)

**Phase 1 过滤规则**:
- 排除粉丝数 < 10 (僵尸号)
- 排除粉丝数 > 100万 (明星/官方号)
- 排除笔记数 < 3 (不活跃)
- 排除最近笔记 > 90天前 (休眠账号)

### 3.3 数据schema

**raw_users.csv**:
```csv
user_id,username,nickname,bio,followers_count,following_count,likes_count,notes_count,profile_url,crawl_time,raw_data_path
```

**filtered_users.csv**:
```csv
user_id,username,quality_score,segment_tags,gender_guess,age_guess,region_guess,content_style,price_sensitivity,scene_preference,kfc_notes_count,total_notes_count,kfc_ratio,notes_sample,crawl_time
```

---

## 四、技术栈选型

### 4.1 核心工具对比

| 工具 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **Playwright** | 功能完整、社区成熟、文档完善 | 学习曲线略陡 | 主采集框架 ✅ |
| **Browser-use** | 专为AI设计、自动推理 | 较新、稳定性待验证 | 复杂交互辅助 |
| **Agent-browser** | 本地集成、有头模式支持 | 功能相对基础 | 简单页面采集 |
| **DrissionPage** | 国产、反爬能力强 | 社区较小 | 备选反爬方案 |

**决策**: 主框架用 **Playwright**，复杂场景结合 **Agent-browser** 技能

### 4.2 完整技术栈

```yaml
# 浏览器自动化
- Playwright (Python) - 主采集引擎
- Chromium ( headed mode ) - 浏览器实例

# 数据处理
- pandas - CSV处理、数据清洗
- numpy - 数值计算
- scikit-learn - 聚类分析

# 内容分析
- jieba - 中文分词
- wordcloud - 词云生成
- regex - 模式匹配

# 反爬与代理
- residential_proxy (optional) - 住宅IP池
- random_user_agent - UA轮换
- playwright-stealth - 特征隐藏

# 存储
- CSV (阶段数据)
- JSON (原始数据)
- SQLite (关系查询)

# 可视化
- matplotlib/seaborn - 统计图表
- pyecharts - 交互式图表
- HTML/CSS - 仪表盘
```

### 4.3 反爬策略 (Critical)

小红书反爬严格，必须实施：

1. **请求频率控制**:
   - 每个账号每小时最多访问 30 个主页
   - 页面间随机延迟: 3-8 秒
   - 模拟真实滚动行为

2. **行为模拟**:
   - 随机鼠标移动轨迹
   - 页面停留时间: 5-15 秒
   - 模拟阅读滚动

3. **账号安全**:
   - 使用备用小号 (非主账号)
   - 准备 2-3 个账号轮换
   - 每日采集上限: 200 个用户/账号

4. **IP策略**:
   - 住宅代理池 (推荐)
   - 或本地网络 + 长时间间隔

---

## 五、实施计划

### 5.1 里程碑

| 阶段 | 任务 | 时间 | 产出 |
|------|------|------|------|
| M1 | 环境搭建 + 工具验证 | Day 1-2 | 可运行的采集脚本 |
| M2 | 小批量测试 (N=50) | Day 3-5 | 测试报告 + 流程优化 |
| M3 | 规则调优 + 评分系统 | Day 6-7 | 评分模型V1 |
| M4 | 批量采集 (N=2000) | Day 8-14 | raw_users.csv |
| M5 | 数据处理 + 评估 | Day 15-17 | filtered_users.csv |
| M6 | 分析报告 + 可视化 | Day 18-20 | 最终交付物 |

### 5.2 测试阶段 (N=50) 重点验证

- [ ] 账号登录态能保持多久
- [ ] 多少请求会触发验证码
- [ ] 主页数据能否完整采集
- [ ] 评分算法是否合理
- [ ] 数据存储结构是否满足分析需求

---

## 六、风险与对策

| 风险 | 可能性 | 影响 | 对策 |
|------|--------|------|------|
| 账号被封 | 高 | 项目中断 | 备用账号、降低频率、IP代理 |
| 反爬升级 | 中 | 采集失败 | 预留技术升级时间、多方案备选 |
| 数据质量差 | 中 | 画像失真 | 严格的评分过滤、人工抽检 |
| 用户隐私变更 | 低 | 数据缺失 | 仅采集公开数据、遵守平台规则 |

---

## 七、待确认事项 (Blocking)

请确认以下问题，以便正式启动：

### 7.1 账号相关
- [ ] 小红书备用账号是否已准备就绪？
- [ ] 是否需要我协助注册/准备账号？
- [ ] 账号登录信息如何提供？（需要cookie或扫码登录）

### 7.2 采集范围
- [ ] 是否采集笔记正文全文？（增加采集时间但提升分析质量）
- [ ] 还是仅采集笔记标题 + 互动数据？（效率优先）

### 7.3 时间预期
- [ ] 是否接受 ~20天 的项目周期？
- [ ] 还是希望压缩到更短时间？（可能影响数据质量）

### 7.4 交付物
- [ ] 用户画像报告是否需要中文/英文双语？
- [ ] 是否需要PPT格式的总结？
- [ ] 原始数据是否需要一并交付？

---

## 八、下一步行动

1. **等待确认**: 解决上述待确认事项
2. **环境准备**: 安装依赖、配置账号
3. **M1启动**: 编写 Phase 1 搜索采集脚本
4. **测试验证**: N=50 小规模测试
5. **迭代优化**: 根据测试结果调整策略
6. **正式采集**: 启动批量采集

---

*文档版本: v2.0*  
*最后更新: 2026-02-06*  
*状态: 待启动 (等待确认)*
