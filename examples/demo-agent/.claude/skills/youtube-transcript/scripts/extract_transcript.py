#!/usr/bin/env python3
"""
YouTube Transcript Extractor
ä¸‹è½½ YouTube è§†é¢‘å­—å¹•å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„ Markdown æ–‡æ¡£
"""

import sys
import re
import subprocess
import os
from pathlib import Path
from urllib.parse import urlparse, parse_qs


def check_yt_dlp():
    """æ£€æŸ¥ yt-dlp æ˜¯å¦å·²å®‰è£…"""
    try:
        subprocess.run(['yt-dlp', '--version'], capture_output=True, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False


def install_yt_dlp():
    """å®‰è£… yt-dlp"""
    print("æ­£åœ¨å®‰è£… yt-dlp...")
    try:
        subprocess.run(['brew', 'install', 'yt-dlp'], check=True)
        print("âœ… yt-dlp å®‰è£…æˆåŠŸ")
        return True
    except subprocess.CalledProcessError:
        print("âŒ å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…: brew install yt-dlp")
        return False


def extract_video_id(url):
    """ä» URL ä¸­æå–è§†é¢‘ ID"""
    parsed = urlparse(url)
    if parsed.netloc in ('youtu.be', 'www.youtu.be'):
        return parsed.path[1:]
    if 'youtube.com' in parsed.netloc:
        query = parse_qs(parsed.query)
        return query.get('v', [None])[0]
    return None


def get_available_subtitles(url):
    """è·å–å¯ç”¨çš„å­—å¹•åˆ—è¡¨"""
    try:
        result = subprocess.run(
            ['yt-dlp', '--list-subs', url],
            capture_output=True,
            text=True,
            timeout=30
        )
        return result.stdout
    except Exception as e:
        print(f"è·å–å­—å¹•åˆ—è¡¨å¤±è´¥: {e}")
        return None


def download_subtitle(url, lang='en', auto_sub=True):
    """ä¸‹è½½å­—å¹•æ–‡ä»¶"""
    video_id = extract_video_id(url)
    if not video_id:
        print("âŒ æ— æ³•æå–è§†é¢‘ ID")
        return None
    
    # æ„å»ºä¸‹è½½å‘½ä»¤
    cmd = [
        'yt-dlp',
        '--write-subs' if not auto_sub else '--write-auto-subs',
        '--sub-langs', lang,
        '--skip-download',
        '--sub-format', 'srt',
        '-o', f'temp_{video_id}.%(ext)s',
        url
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        
        # æŸ¥æ‰¾ä¸‹è½½çš„å­—å¹•æ–‡ä»¶
        temp_files = list(Path('.').glob(f'temp_{video_id}*.srt'))
        if temp_files:
            return str(temp_files[0])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ‰‹åŠ¨å­—å¹•
        if auto_sub:
            print("è‡ªåŠ¨å­—å¹•ä¸å¯ç”¨ï¼Œå°è¯•æ‰‹åŠ¨å­—å¹•...")
            return download_subtitle(url, lang, auto_sub=False)
        
        return None
        
    except Exception as e:
        print(f"ä¸‹è½½å­—å¹•å¤±è´¥: {e}")
        return None


def parse_srt(srt_file):
    """è§£æ SRT æ–‡ä»¶å¹¶æå–æ–‡æœ¬"""
    with open(srt_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # åˆ†å‰²æ¡ç›®
    entries = re.split(r'\n\n+', content.strip())
    
    parsed_entries = []
    for entry in entries:
        lines = entry.strip().split('\n')
        if len(lines) >= 3:
            # ç¬¬ä¸€è¡Œæ˜¯åºå·ï¼Œç¬¬äºŒè¡Œæ˜¯æ—¶é—´æˆ³ï¼Œå‰©ä¸‹çš„æ˜¯æ–‡æœ¬
            time_line = lines[1]
            text_lines = lines[2:]
            
            # è§£ææ—¶é—´
            match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3})', time_line)
            if match:
                start_time = match.group(1)
                text = ' '.join(text_lines)
                # æ¸…ç†æ ‡è®°
                text = re.sub(r'\[.*?\]', '', text)  # ç§»é™¤ [Music] ç­‰
                text = re.sub(r'\s+', ' ', text).strip()
                if text and not text.isdigit() and len(text) > 2:
                    parsed_entries.append((start_time, text))
    
    return parsed_entries


def merge_paragraphs(entries, max_gap_seconds=3, max_para_length=500):
    """å°†çŸ­å¥åˆå¹¶æˆæ®µè½"""
    paragraphs = []
    current_para = ""
    last_time = None
    
    for time_str, text in entries:
        # è®¡ç®—æ—¶é—´é—´éš”
        if last_time:
            current_secs = sum(x * int(t) for x, t in 
                             zip([3600, 60, 1], time_str.split(',')[0].split(':')))
            last_secs = sum(x * int(t) for x, t in 
                          zip([3600, 60, 1], last_time.split(',')[0].split(':')))
            gap = current_secs - last_secs
        else:
            gap = 0
        
        # å†³å®šæ˜¯å¦å¼€å§‹æ–°æ®µè½
        if gap >= max_gap_seconds or len(current_para) >= max_para_length:
            if current_para:
                paragraphs.append(current_para)
            current_para = text
        else:
            if current_para:
                current_para += " " + text
            else:
                current_para = text
        
        last_time = time_str
    
    if current_para:
        paragraphs.append(current_para)
    
    # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
    return [p for p in paragraphs if len(p) > 30]


def generate_markdown(url, title, paragraphs, output_file=None):
    """ç”Ÿæˆ Markdown æ–‡æ¡£"""
    video_id = extract_video_id(url)
    
    if not output_file:
        safe_title = re.sub(r'[^\w\s-]', '', title).strip().replace(' ', '_')[:50]
        output_file = f"{safe_title}_æ–‡å­—ç¨¿.md"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ¬ {title}\n\n")
        f.write(f"> è§†é¢‘æ–‡å­—ç¨¿\n\n")
        f.write(f"ğŸ“º **åŸè§†é¢‘**ï¼š{url}\n")
        f.write(f"ğŸ• **ç”Ÿæˆæ—¶é—´**ï¼š{__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        f.write(f"ğŸ“ **æ®µè½æ•°**ï¼š{len(paragraphs)}\n\n")
        f.write("---\n\n")
        
        f.write("## ğŸ“– å®Œæ•´å†…å®¹\n\n")
        for i, para in enumerate(paragraphs, 1):
            f.write(f"{para}\n\n")
        
        f.write("---\n\n")
        f.write("*æœ¬æ–‡å­—ç¨¿ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œå»ºè®®é…åˆåŸè§†é¢‘è§‚çœ‹*\n")
    
    return output_file


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python extract_transcript.py <YouTube URL> [è¾“å‡ºæ–‡ä»¶å]")
        print("ç¤ºä¾‹: python extract_transcript.py 'https://www.youtube.com/watch?v=xxxxx'")
        sys.exit(1)
    
    url = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    print(f"ğŸ¯ å¤„ç†è§†é¢‘: {url}")
    
    # æ£€æŸ¥ yt-dlp
    if not check_yt_dlp():
        print("âš ï¸  yt-dlp æœªå®‰è£…")
        if not install_yt_dlp():
            sys.exit(1)
    
    # è·å–è§†é¢‘ä¿¡æ¯
    print("ğŸ“‹ è·å–è§†é¢‘ä¿¡æ¯...")
    try:
        result = subprocess.run(
            ['yt-dlp', '--print', '%(title)s', '--no-download', url],
            capture_output=True,
            text=True,
            timeout=30
        )
        title = result.stdout.strip()
        print(f"ğŸ¬ è§†é¢‘æ ‡é¢˜: {title}")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•è·å–æ ‡é¢˜: {e}")
        title = "Untitled"
    
    # ä¸‹è½½å­—å¹•
    print("â¬‡ï¸  ä¸‹è½½å­—å¹•...")
    srt_file = download_subtitle(url)
    
    if not srt_file:
        print("âŒ æ— æ³•ä¸‹è½½å­—å¹•ï¼Œè¯¥è§†é¢‘å¯èƒ½æ²¡æœ‰å­—å¹•")
        sys.exit(1)
    
    print(f"âœ… å­—å¹•å·²ä¸‹è½½: {srt_file}")
    
    # è§£æå­—å¹•
    print("ğŸ“ è§£æå­—å¹•...")
    entries = parse_srt(srt_file)
    print(f"âœ… è§£æå®Œæˆï¼Œå…± {len(entries)} æ¡å­—å¹•")
    
    # åˆå¹¶æ®µè½
    print("ğŸ§© åˆå¹¶æ®µè½...")
    paragraphs = merge_paragraphs(entries)
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå…± {len(paragraphs)} ä¸ªæ®µè½")
    
    # ç”Ÿæˆ Markdown
    print("ğŸ“„ ç”Ÿæˆ Markdown...")
    output = generate_markdown(url, title, paragraphs, output_file)
    print(f"âœ… å®Œæˆï¼æ–‡ä»¶ä¿å­˜ä¸º: {output}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if os.path.exists(srt_file):
        os.remove(srt_file)
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")


if __name__ == '__main__':
    main()
